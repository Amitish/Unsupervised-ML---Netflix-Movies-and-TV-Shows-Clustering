{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "ArJBuiUVfxKd",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amitish/Unsupervised-ML---Netflix-Movies-and-TV-Shows-Clustering/blob/main/Unsupervised_ML_Netflix_Movies_and_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Unsupervised ML - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix is an American subscription video on-demand over-the-top streaming service. The service primarily distributes original and acquired films and television shows from various genres, and it is available internationally in multiple languages.\n",
        "\n",
        "Get ready to unlock hidden potential! This project meticulously addresses data quality issues, transforming raw information into a powerful resource.\n",
        "\n",
        "This project harnesses machine learning to group Netflix's vast library of 7,000+ movies and shows by content, helping users discover hidden gems and navigate the platform effortlessly.\n",
        "\n",
        "To gain insights into the diverse content offered by Netflix, we are going to analyze the dataset containing details about movies and TV shows. We will employee descriptive statistics to understand the distribution of key variables and create visualizations like scatterplots, histograms, line charts, heatmaps etc to explore relationships between them.\n",
        "\n",
        "This multi-faceted approach will help us uncover valuable patterns and trends within the dataset. Moreover we will identify the key anamolies and try to work upon it.\n",
        "\n",
        "A concluding statement will not only summarize our findings but also empower audiences to derive value and fuel their own projects with these actionable insights.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Amitish/Unsupervised-ML---Netflix-Movies-and-TV-Shows-Clustering"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix welcomes new members with a personalized onboarding journey, guiding them through account setup, preference selection, and curated recommendations based on their viewing history. Walking through such a huge textual data can be impractical and resource-intensive taking in lot of time and efforts."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "\n",
        "# visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# nltk library\n",
        "import nltk\n",
        "nltk.download('stopwords')  # downloads stopwords like \"a\",\"an\",\"the\"\n",
        "nltk.download('punkt')      # for tokenization - breaking text into individual sentence\n",
        "\n",
        "# downloads stopwords from nltk library if corpus is available\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# used for matching of string ascii, punctuations, digits\n",
        "import string\n",
        "\n",
        "# Importing regex library for comparison\n",
        "import re\n",
        "\n",
        "# For web scrapping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Method for reducing words to their base forms\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# More accurate than simple stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import TfidfVectorizer )for counting word occurance)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# Creating and displaying formatted tables (beautify)\n",
        "from tabulate import tabulate\n",
        "\n",
        "# KElbowVisualizer\n",
        "from yellowbrick.cluster import KElbowVisualizer,SilhouetteVisualizer\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "\n",
        "# Importing clustering Evaluation metrics\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Install yellowbrick library\n",
        "!pip install yellowbrick\n",
        "!pip install contractions\n",
        "\n",
        "# For Cross-Validation and Hyperparameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Importing algorithams for building model\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "from google.colab import drive                                    # Links the Google drive with Colab notebook, to extract the desired dataset\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = pd.read_csv(\"/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")   # Extracting datafile"
      ],
      "metadata": {
        "id": "pdQluNUfxD1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "net"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "no_of_rows = net.shape[0]\n",
        "no_of_columns = net.shape[1]\n",
        "\n",
        "print(\"no_of_rows: \",no_of_rows)\n",
        "print(\"no_of_columns: \",no_of_columns)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "net.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "duplicates = net.duplicated().sum()\n",
        "duplicates\n",
        "\n",
        "# 0 indicates that no duplicate rows found in entire dataset"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "null = net.isnull().sum().reset_index()\n",
        "null\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "nulls= pd.DataFrame({'Column Name':net.columns,'Null Values':net.isnull().sum(),'Percentage %':round(net.isnull().sum()*100/len(net),2)})\n",
        "nulls.set_index('Column Name').sort_values(by='Percentage %', ascending = False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Findings by far:\n",
        "1. No duplicate rows identified in the dataset.\n",
        "2. **director, cast and country** column holds the most no.of nulls whereas **date_added** and **rating** column having the least.\n",
        "3. **release_year** holds numeric data where all other columns are categorical.\n",
        "4. As data cleaning requires replacing null values but replacing null values can sometime mislead the dataset.\n",
        "5. So, it must be worked upon cautiously only were its required."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "net.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "net.describe()\n",
        "\n",
        "# since release_year is the only numeric datatype column"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = [i for i in net.describe(include=\"object\")]\n",
        "print(\">>>\", categorical)\n",
        "print(\" \")\n",
        "print(\"No.of Categorical columns: \", len(categorical))"
      ],
      "metadata": {
        "id": "5O-bmstqHYMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric = [j for j in net.columns if j not in categorical]\n",
        "print(\">>>\", numeric)\n",
        "print(\" \")\n",
        "print(\"No.of Numeric columns: \", len(numeric))"
      ],
      "metadata": {
        "id": "lsR1Bt1YH8iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that:\n",
        "1. No.of rows x columns =  **7787 x 12**\n",
        "2. No.of categorical columns = **11**\n",
        "3. No.of numeric columns = **1**"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for i in net.columns:\n",
        "  d = net[i].unique()                           # unique() to get values and nunique() to get number\n",
        "  print(\"Column ---------\", i)\n",
        "  print(d)\n",
        "  print(\"No.of unique values ---------\", len(d))\n",
        "  print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "net.fillna({\"director\": \"unknown\", \"cast\": \"unknown\", \"country\": \"unknown\"}, inplace=True)\n",
        "\n",
        "net.dropna(subset=[\"date_added\", \"rating\"], inplace=True)\n",
        "\n",
        "# Converting the data types of features date_added and release_year to the appropriate data types\n",
        "net['date_added'] = net['date_added'].str.strip()\n",
        "net['date_added'] = pd.to_datetime(net['date_added'], format='%B %d, %Y', errors='coerce')\n",
        "net.release_year = net.release_year.astype('int64')\n",
        "\n",
        "# Renaming name of column listed_in to genre\n",
        "net.rename = net.rename(columns={'listed_in':'genre'}, inplace=True)\n",
        "\n",
        "# Breaking down the date_added column based on dd/mm/yy\n",
        "net['year_added']= net['date_added'].dt.year\n",
        "net['month_added']= net['date_added'].dt.month\n",
        "net['day_added']= net['date_added'].dt.day\n",
        "\n",
        "# Deleting column date_added\n",
        "net.drop(\"date_added\", axis=1, inplace=True)\n",
        "\n",
        "#convert int32 into int64\n",
        "net['year_added'] = net['year_added'].astype(np.int64)\n",
        "net['month_added']= net['month_added'].astype(np.int64)\n",
        "net['day_added']= net['day_added'].astype(np.int64)\n",
        "\n",
        "# Partitioning and creating new dataset based on type of show\n",
        "tv_shows_data = net[net[\"type\"]=='TV Show']\n",
        "movie_shows_data = net[net[\"type\"]=='Movie']"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.isnull().sum().reset_index()"
      ],
      "metadata": {
        "id": "CCLHEtS9y0pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Then we performed **isnull()** to find null values and concluded that we have **5** columns with some null values.\n",
        "2. We initiated **drop_duplicates()** method but didn't find any duplicate values in the dataset.\n",
        "3. The 2 coulmns i.e., **date_added** and **rating** had very less number of null values, so we removed those null values.\n",
        "4. The 3 columns i.e., **director, cast** and **country** had huge number of null values so dropping these columns couldn't be afforded hence we replaced null values with **\"unknown\"**.\n",
        "5. Converted the data types of features date_added and release_year to the appropriate data types.\n",
        "6. Renamed name of column **listed_in** to **genre**.\n",
        "7. Broke down the **date_added** column based on **dd/mm/yy** and deleted the previous column.\n",
        "8. We now have **14 rows** instead of 12.\n",
        "9. Partitioned and creating new dataset based on type of show."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.shape"
      ],
      "metadata": {
        "id": "mXv2eV9EUaK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "v5vkb4aRUYSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(x=net[\"type\"])\n",
        "\n",
        "for p in ax.patches:\n",
        "    count = p.get_height()  # Get the count for each bar\n",
        "    x = p.get_x() + p.get_width() / 2  # Get the center x-coordinate of each bar\n",
        "    y = count\n",
        "    ax.annotate(f\"{count}\", (x, y), ha='center')\n"
      ],
      "metadata": {
        "id": "mw36lo4hOYvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The chart reflects the count of show type that is available. This gives a kind of understanding as to which show is most displayed and hence helps the user in making the right decision before watching.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart gives a count of Show Type:\n",
        "- Movie : **5372**\n",
        "- TV Show : **2398**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can drive insite from the chart above that **Movie** is the most displayed/watched show type. This will eventually give users an idea about the popularity of show type  and hence assist them in making right choice.\n",
        "\n",
        "- The insite will definitely not just benefit users in their decision making process but in turn will also benefit directors to know about the users interest."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "shows_produced = net[\"director\"].value_counts()\n",
        "\n",
        "# Filter out \"unknown\" values, keeping remaining top 10\n",
        "shows_produced = shows_produced.loc[shows_produced.index != \"unknown\"].head(10).sort_values(ascending=True)\n",
        "\n",
        "colors = [\"red\", \"blue\", \"pink\", \"black\", \"green\"]\n",
        "\n",
        "plt.barh(shows_produced.index, shows_produced.values, color=colors)\n",
        "\n",
        "for i, v in enumerate(shows_produced.values):\n",
        "    plt.text(v + 0.05, i, str(v), va=\"center\")\n",
        "\n",
        "plt.title(\"Top 10 Directed Shows\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Directors\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The reason of selecting this chart is to get an overview of top 10 Directors  with most number of shows count.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can state that:\n",
        "1. **Raul Campos, Jan Suter** has directed most no.of shows i.e, **18** amongst top 10.\n",
        "2. **Robert Rodriguez** has directed **8** shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes indeed the gained insights will help in creating positive business impact as it portrays an understanding of maximum shows directed by a Director. Based on the experience right director can be approached.\n",
        "\n",
        "- As such no negative growth can be fetched from above dataset."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "avg_year = net[\"release_year\"].mean()\n",
        "\n",
        "plt.hist(net[\"release_year\"], bins=20, edgecolor=\"black\")\n",
        "\n",
        "plt.axvline(x=avg_year, color=\"red\", linestyle=\"dashed\", linewidth=1, label=\"Average Release Year\")\n",
        "\n",
        "plt.title(\"Number of Shows Released\")\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SUl1_XhCGDCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Selecting the above chart conveys a visual of how shows have been released in an interval of time on an average."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Following information can be driven out:\n",
        "1. Release of shows has seen an **incremental** growth with time.\n",
        "2. **Average** no.of shows releases are in the year **2013**.\n",
        "3. **Maximum** no.of shows releases are in the year **2021**."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above insight will to some extend help in gaining positive business impact as we can proclaim with every increasing year the no.of shows release is increasing.\n",
        "- As of we can't see any negative growth from the data above as all the figure that we have seems to be upto the mark, adding some positivity to the data."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "ratings = net[\"rating\"].value_counts()\n",
        "\n",
        "colors = [\"red\", \"blue\", \"pink\", \"black\", \"green\"]\n",
        "plt.bar(ratings.index, ratings.values, color=colors)\n",
        "\n",
        "# Add count labels above each bar\n",
        "for i, v in enumerate(ratings.values):\n",
        "    plt.text(i, v + 50, str(v), ha=\"center\")\n",
        "\n",
        "# Customize the chart\n",
        "plt.title(\"Shows Rating Distribution\")\n",
        "plt.xlabel(\"Ratings\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The logic of driving bar chart is to conceive the insight of ratings count being majorly provided."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above graph states that:\n",
        "1. **TV-MA** is the **highest** rating in the list with count of **2861**.\n",
        "2. **NC-17** is almost the **least** rating in the list with count of **3**."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In short we can for sure say that the above analysis will lay a positive impact on business, as they will get a clear picture of ratings count.\n",
        "- One negative point is that lesser the rating lesser is the popularity of the show."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "shows_share = net[\"type\"].value_counts()\n",
        "\n",
        "plt.pie(shows_share.values, autopct='%1.1f%%', labels=shows_share.index)\n",
        "\n",
        "plt.title(\"Shows Share Percentage\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The motive of selecting this specific chart is to get a clear picture of show type distribyution."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the above chart we can say that:\n",
        "1. **Movie** show type constitutes for **69.1%** overall.\n",
        "2. **TV** Show constitutes for **30.9%**."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes the gained insight will surely create a positive business impact as user will get to know the weightage shared by each show type.\n",
        "- Moreover directors will also have a better understanding of the public demand."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "shows_prod = tv_shows_data[\"director\"].value_counts()\n",
        "\n",
        "# Filter out \"unknown\" values, keeping remaining top 10\n",
        "shows_prod = shows_prod.loc[shows_prod.index != \"unknown\"].head(10).sort_values(ascending=True)\n",
        "\n",
        "colors = [\"red\", \"blue\", \"pink\", \"black\", \"green\"]\n",
        "\n",
        "plt.barh(shows_prod.index, shows_prod.values, color=colors)\n",
        "\n",
        "for i, v in enumerate(shows_prod.values):\n",
        "    plt.text(v + 0.05, i, str(v), va=\"center\")\n",
        "\n",
        "plt.title(\"Top 10 Directed TV Shows\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Directors\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The label graph reflects distribution of Top 10 TV shows directed by Directors."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can state following conclusion:\n",
        "1. **Alastair** has directed **3** TV shows amongst top 10.\n",
        "2. There are many director with 2 or less no.of TV shows directed.\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- More no.of TV shows directed gives an idea about how experienced the director is a particular TV show type.\n",
        "- The data doesnt interpret any negative growth as such."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "mov_prod = movie_shows_data[\"director\"].value_counts()\n",
        "\n",
        "# Filter out \"unknown\" values, keeping remaining top 10\n",
        "mov_prod = mov_prod.loc[mov_prod.index != \"unknown\"].head(10).sort_values(ascending=True)\n",
        "\n",
        "colors = [\"black\", \"green\", \"orange\", \"blue\", \"grey\"]\n",
        "\n",
        "plt.barh(mov_prod.index, mov_prod.values, color=colors)\n",
        "\n",
        "for i, v in enumerate(mov_prod.values):\n",
        "    plt.text(v + 0.2, i, str(v), va=\"center\")\n",
        "\n",
        "plt.title(\"Top 10 Directed Movies\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Directors\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The label graph potrays distribution of Top 10 TV movies directed by respective directors.\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Raul** and **Jan** has directed most no.of movies i.e., **18** each.\n",
        "2. **Johnnie** has directed **8** movies."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The number of films directed by a director can give some indication of their experience in a particular movie type.\n",
        "- By far we don't find any insights that lead to negative growth."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "mov_con = movie_shows_data[\"country\"].value_counts()\n",
        "\n",
        "# Filter out \"unknown\" values, keeping remaining top 10\n",
        "mov_con = mov_con.loc[mov_con.index != \"unknown\"].head(10).sort_values(ascending=True)\n",
        "\n",
        "colors = [\"black\", \"red\", \"grey\"]\n",
        "\n",
        "plt.barh(mov_con.index, mov_con.values, color=colors)\n",
        "\n",
        "for i, v in enumerate(mov_con.values):\n",
        "    plt.text(v + 0.2, i, str(v), va=\"center\")\n",
        "\n",
        "plt.title(\"Top 10 Countries with most Movies Production\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Countries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This visualization highlights the movie with the highest production count."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can conclude that:\n",
        "1. Most no.of movies production Country is **United States** with count **1847**.\n",
        "2. **India** is the **2nd** largest movie producing Country with count **852**."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The stats clearly shows that United States is the highest movie producing country which reflects its the first choice for every director.\n",
        "- Apart from United States and India other countries need to work on production capacity."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "tv_show_con = tv_shows_data[\"country\"].value_counts()\n",
        "\n",
        "# Filter out \"unknown\" values, keeping remaining top 10\n",
        "tv_show_con = tv_show_con.loc[tv_show_con.index != \"unknown\"].head(10).sort_values(ascending=True)\n",
        "\n",
        "colors = [\"blue\", \"green\", \"orange\"]\n",
        "\n",
        "plt.barh(tv_show_con.index, tv_show_con.values, color=colors)\n",
        "\n",
        "for i, v in enumerate(tv_show_con.values):\n",
        "    plt.text(v + 0.2, i, str(v), va=\"center\")\n",
        "\n",
        "plt.title(\"Top 10 Countries with most TV Shows Production\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Countries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This graph pinpoints the countries TV shows with the highest number involved in its production."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can deduce following conclusion:\n",
        "1. **United States** is the highest TV show producing country with count of **699**.\n",
        "2. **United Kingdom** is **2nd** highest TV show producing country with count of **203**."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Statistics reveal that the United States leads the world in TV Show  production, suggesting it may be a preferred destination for many directors.\n",
        "- Except United States all other countries have shown a pretty small no.of production capacity."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "plt.subplots(figsize=(10,8))\n",
        "\n",
        "casting = net[net['cast'] != 'unknown']\n",
        "\n",
        "wordcloud = WordCloud(background_color='white').generate(','.join(casting.cast))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The word cloud gives a visual glimpse of which actor/actress has performed in most no.of shows."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can deduce following information from above visuals:\n",
        "1. The word with bigger text size depicts that those casts has been casted most no.of movies like Michael, Lee etc.\n",
        "2. The ones with smaller text size depicts that those casts has been casted in less no.of movies  like Tim, Yu etc."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes surely the insight will help directors to make the right decision while casting the actor/actress based on their experience.\n",
        "- No negative insight can be predicted from the above visual."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "tv_shows_yr = tv_shows_data['release_year'].value_counts().sort_index(ascending=False)\n",
        "movie_shows_yr = movie_shows_data['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "movie_shows_yr.plot(figsize=(10, 5), linewidth=2, color='red')\n",
        "tv_shows_yr.plot(figsize=(10, 5), linewidth=2, color='blue')\n",
        "plt.xlabel(\"Years\", labelpad=5)\n",
        "plt.ylabel(\"Count\", labelpad=5)\n",
        "plt.title(\"Movies vs TV_shows Release Year Analysis\")\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The line chart shows time trend analysis of movies vs TV shows release year."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is clear from above graph that:\n",
        "1. Both TV shows and movie release year has seen an incremental growth over years.\n",
        "2. 2020-2021 has seen maximum releases for both type of shows.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Definitely the above states delivers a positive buiness impact by providing a clear picture of shows release year.\n",
        "- Release year for both type of TV shows has seen an incremental groth over years so no negative growth can be stated."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "palette = sns.color_palette(\"Set1\")\n",
        "sns.countplot(x=\"month_added\", hue=\"type\", lw=2, data=net, palette=palette)\n",
        "\n",
        "plt.title(\"Movies vs TV_shows added every month\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The bar chart above give a pictorial representation of how shows are added month on month."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can state from above information that:\n",
        "1. Movie show type has shown more no.of month on month addition as compared to TV show type.\n",
        "2. Both show type has seem a fluctuating trend in addition."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can signify from above stats that though for both the show types month on month addition is not stable, it shows fluctuating result but movie show type has seen more additions.\n",
        "- The only near to negative fact about the stats is that TV show type has seen lesser growth as compared to movie show type.\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.title('Top 10 Genre of Shows Type')\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "sns.countplot(y=net['genre'], data=net, order=net['genre'].value_counts().index[0:10], palette=palette)\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The label chart shows how choice of genre is prferred amongst directors."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above figure states that:\n",
        "1. Documenatries is the best choice of directors amongst all genre type.\n",
        "2. There is a close call between stand up comedy and dramas genre type."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The gained insight will for sure help creating a positive businesss impact from directors point of view as they can focus on the genre which is mostly preferred.\n",
        "- As far no negative insight can be deduced from the above states."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "sns.heatmap(data=net[[\"release_year\",\"year_added\",\"month_added\", \"day_added\"]].corr(), annot= True)\n",
        "\n",
        "plt.title(\"Overall detailed Analysis\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The selection of heatmap chart is to give a clear insight of how each and every series in the data set are linked to each other, how collaborative the bonds are and how loose are they."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can say from above observation that:\n",
        "1. The one's with lightest color and indicated with numeric value 1 shares the strongest bond i.e., the series when compared with itself serves the deep collaboration.\n",
        "2. As the color keeps darkening, the strength of the bonds keeps on deteriorating and the numeric values justifies it all.\n",
        "3. The series who's joint venture depicts the darkest color and have least numeric value claims to have the weakest bond."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(net)\n",
        "\n",
        "plt.title(\"Overall detailed Analysis\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The motive of selecting pair plot is to get a summarized insight of relationship shared between different type of columns in the dataset."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Following analysis can be drawn from the above data:\n",
        "1. Out of all the columns we have extracted out the ones which are are numeric in nature and add some meaningful data to the analysis.\n",
        "2. The relationship charts give clear highlights of how each column in the dataset shares a relation with another.\n",
        "3. The peaks in the chart signifies a strong relation when a specified column intersects with itself."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Hypothetical Statement 1:** Viewer Rating and Mature Content\n",
        "2. **Hypothetical Statement 2:** Release Year Impact on Show Volume\n",
        "3. **Hypothetical Statement 3:** Genre of Movies"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hypothesis:** The number of shows released on Netflix has significantly increased **after 2010** compared to **before 201**0.\n",
        "\n",
        "- **Null Hypothesis (Ho)**: The number of shows released **before 2010** is the same as or more than the number released after 2010.\n",
        "- **Alternative Hypothesis (Ha)**: The number of shows released **after 2010** is more than the number released before 2010"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows released after 2010\n",
        "net_after_2010 = net[net[\"year_added\"]>2010]\n",
        "print(\"Shows_released_after_2010: \", net_after_2010.shape[0])"
      ],
      "metadata": {
        "id": "DTF0ZVexT2gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows released before 2010\n",
        "net_before_2010 = net[net[\"year_added\"]<=2010]\n",
        "print(\"Shows_released_before_2010: \", net_before_2010.shape[0])"
      ],
      "metadata": {
        "id": "g5nKC5OsVg2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1 = net_after_2010[\"year_added\"].mean()\n",
        "print(\"mean_net_after_2010: \", round(mean1,0))\n",
        "mean2 = net_before_2010[\"year_added\"].mean()\n",
        "print(\"mean_net_before_2010: \", round(mean2,0))\n",
        "overall_mean = net[\"year_added\"].mean()\n",
        "print(\"mean_overall: \",  round(overall_mean,0))"
      ],
      "metadata": {
        "id": "PBiU4564gfPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# We will perform Z Test Statistics\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "ts= net.shape[0]\n",
        "\n",
        "try:\n",
        "    count = np.array([net_after_2010, net_before_2010])\n",
        "    print(\"Array:\", count)\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "nobs = np.array([ts, ts])  # Total observations\n",
        "\n",
        "stat, pval = proportions_ztest(count, nobs, alternative='larger')\n",
        "\n",
        "print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print('p-value:', pval)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- He have choosen **Z Test** to determine the **p value** for the above hypothesis.\n",
        "- The **p value** for the above scenerio rolls out to be **0.5**.\n",
        "- **p value 0.5** clearly stats that its **too high** and falls under **type II error**.\n",
        "- Moreover from above observation its visible that **population mean** is close to **Ha hypothesis**.\n",
        "- Hence we conclude that based on the result, we will **reject the Ho hypothesis**."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Z-score hypothesis** testing could indeed be suited for **analyzing the impact** of a specific year (2010) on the volume of shows released on Netflix, as outlined in your previous scenario about whether there has been a significant increase in the number of shows released after 2010 compared to before.\n",
        "- The **Z-test** is generally **preferred** when the sample size is large (**typically n > 30**).\n",
        "- In hypothesis testing, converting the **difference between** the **sample mean** and the **population mean** into a Z-score allows for easy comparison against standard normal distribution values."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hypothesis:** Shows classified as \"**TV-MA**\" (Mature Audience) have a **higher** average viewer rating than shows classified as \"**TV-PG**\" (Parental Guidance).\n",
        "\n",
        "- **Null Hypothesis (Ho)**: Mean viewer rating for **TV-MA** shows **equals** the mean viewer rating for **TV-PG** shows\n",
        "- **Alternative Hypothesis (Ha)**: Mean viewer rating for **TV-MA** shows is **greater than** the mean viewer rating for **TV-PG** shows."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rating count for TV_MA\n",
        "net_rating_TV_MA = net[net[\"rating\"] == \"TV-MA\"]\n",
        "print(\"net_rating_TV_MA: \", net_rating_TV_MA.shape[0])"
      ],
      "metadata": {
        "id": "3-oHoz-_jqL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rating count for TV_PG\n",
        "net_rating_TV_PG = net[net[\"rating\"] == \"TV-PG\"]\n",
        "print(\"net_rating_TV_PG: \", net_rating_TV_PG.shape[0])"
      ],
      "metadata": {
        "id": "o7PKwpTQkV7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_mean_rating_TV_MA = 2861/net.shape[0]\n",
        "print(\"net_mean_rating_TV_MA: \", round(net_mean_rating_TV_MA, 2))\n",
        "net_mean_rating_TV_PG = 804/net.shape[0]\n",
        "print(\"net_mean_rating_TV_PG: \", round(net_mean_rating_TV_PG,2))\n",
        "mean2 = (2861+804)/net.shape[0]\n",
        "print(\"Overall_mean: \", round(mean2,2))"
      ],
      "metadata": {
        "id": "LeVpSYAUkrEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# We will perform Z Test Statistics\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "ts= net.shape[0]\n",
        "\n",
        "try:\n",
        "    count = np.array([net_rating_TV_MA, net_rating_TV_PG])\n",
        "    print(\"Array:\", count)\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "nobs2 = np.array([ts, ts])  # Total observations\n",
        "\n",
        "stat2, pval2 = proportions_ztest(count, nobs, alternative='larger')\n",
        "\n",
        "print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print('p-value:', pval2)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- He have choosen **Z Test** to calculate the **p value** for the above testing.  \n",
        "- **p value 0.5** points out to be **too high** and falls under **type II error**.\n",
        "- Moreover from above observation its visible that **population mean** is close to **Ha hypothesis**.\n",
        "- Hence we conclude that based on the result, we will **reject the Ho hypothesis**."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Z-score hypothesis** testing could indeed be suited for **analyzing the rating count** of specifically 2 types of rating i.e., \"**TV-MA**\" (Mature Audience) and \"**TV-PG**\" (Parental Guidance).\n",
        "- The **Z-test** is generally **preferred** when the sample size is large (**typically n > 30**).\n",
        "- In hypothesis testing, converting the **difference between** the **sample mean** and the **population mean** into a Z-score allows for easy comparison against standard normal distribution values."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hypothesis:** The average count of movies labeled as \"**Dramas**\" is less than the average count of movies labeled as \"**Comedies**.\"\n",
        "\n",
        "- **Null Hypothesis (Ho)**: The average count of **Dramas** is the **same** more than **Comedies**.\n",
        "- **Alternative Hypothesis (Ha)**: The average count of **Dramas** is same or less than **Comedies**."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before performing any hypothesis testing we will first explode the genre column and seggregate all the genres seperately\n",
        "\n",
        "net2= net.copy()\n",
        "net2[\"genre\"] = net2[\"genre\"].str.split(\",\")\n",
        "net2 = net2.explode('genre')\n",
        "net2[\"genre\"] = net2[\"genre\"].apply(lambda x: x.strip())\n",
        "net2[\"genre\"]"
      ],
      "metadata": {
        "id": "zo5YZScy3BTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = pd.DataFrame(net2)"
      ],
      "metadata": {
        "id": "thJ-3gfO6Uj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count for genre Dramas\n",
        "net_2_genre_dramas = net_2[net_2[\"genre\"] == \"Dramas\"]\n",
        "print(\"net_2_genre_dramas: \", net_2_genre_dramas.shape[0])"
      ],
      "metadata": {
        "id": "lDP7d93p55oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count for genre Comedies\n",
        "net_2_genre_comedy = net_2[net_2[\"genre\"] == \"Comedies\"]\n",
        "print(\"net_2_genre_comedy: \", net_2_genre_comedy.shape[0])"
      ],
      "metadata": {
        "id": "I2A8lXPb8Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_mean_dramas = 2105/net.shape[0]\n",
        "print(\"net_mean_dramas: \", round(net_mean_dramas, 2))\n",
        "net_mean_comedy = 1471/net.shape[0]\n",
        "print(\"net_mean_comedy: \", round(net_mean_comedy,2))\n",
        "mean2 = (2105+1471)/net.shape[0]\n",
        "print(\"Overall_mean: \", round(mean2,2))"
      ],
      "metadata": {
        "id": "xu2zToeg8k3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# We will perform Z Test Statistics\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "ts= net.shape[0]\n",
        "\n",
        "try:\n",
        "    count = np.array([net_2_genre_comedy, net_2_genre_dramas])\n",
        "    print(\"Array:\", count)\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "nobs3 = np.array([ts, ts])  # Total observations\n",
        "\n",
        "stat3, pval3 = proportions_ztest(count, nobs, alternative='larger')\n",
        "\n",
        "print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print('p-value:', pval3)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- He have choosen **Z Test** to determine the **p value** for the above hypothesis.\n",
        "- The **p value** for the above scenerio rolls out to be **0.5**.\n",
        "- **p value 0.5** clearly stats that its **too high** and falls under **type II error**.\n",
        "- Moreover from above observation its visible that **population mean** is close to **Ho hypothesis**.\n",
        "- Hence we conclude that based on the result, we will **fail to reject the Ho hypothesis**."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Z-score hypothesis** testing could indeed be suited for **analyzing the count** of specific genre like comedies and dramas.\n",
        "- The **Z-test** is generally **preferred** when the sample size is large (**typically n > 30**).\n",
        "- In hypothesis testing, converting the **difference between** the **sample mean** and the **population mean** into a Z-score allows for easy comparison against standard normal distribution values."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netflix = pd.read_csv(\"/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")"
      ],
      "metadata": {
        "id": "zs80lgAADCZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "netflix.fillna({\"director\": \"unknown\", \"cast\": \"unknown\", \"country\": \"unknown\"}, inplace=True)\n",
        "\n",
        "netflix.dropna(subset=[\"date_added\", \"rating\"], inplace=True)\n",
        "\n",
        "# Converting the data types of features date_added and release_year to the appropriate data types\n",
        "netflix['date_added'] = netflix['date_added'].str.strip()\n",
        "netflix['date_added'] = pd.to_datetime(netflix['date_added'], format='%B %d, %Y', errors='coerce')\n",
        "netflix.release_year = netflix.release_year.astype('int64')\n",
        "\n",
        "# Renaming name of column listed_in to genre\n",
        "netflix.rename(columns={'listed_in':'genre'}, inplace=True)\n",
        "\n",
        "# Breaking down the date_added column based on dd/mm/yy\n",
        "netflix['year_added']= netflix['date_added'].dt.year\n",
        "netflix['month_added']= netflix['date_added'].dt.month\n",
        "netflix['day_added']= netflix['date_added'].dt.day\n",
        "\n",
        "# Deleting column date_added\n",
        "netflix.drop(\"date_added\", axis=1, inplace=True)\n",
        "\n",
        "#convert int32 into int64\n",
        "netflix['year_added'] = netflix['year_added'].astype(np.int64)\n",
        "netflix['month_added']= netflix['month_added'].astype(np.int64)\n",
        "netflix['day_added']= netflix['day_added'].astype(np.int64)\n",
        "\n",
        "# Partitioning and creating new dataset based on type of show\n",
        "tv_shows_data1 = netflix[netflix[\"type\"]=='TV Show']\n",
        "movie_shows_data1 = netflix[netflix[\"type\"]=='Movie']"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.isnull().sum().reset_index()"
      ],
      "metadata": {
        "id": "KGmGstULC0CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Then we performed **isnull()** to find null values and concluded that we have **5** columns with some null values.\n",
        "2. We initiated **drop_duplicates()** method but didn't find any duplicate values in the dataset.\n",
        "3. The 2 coulmns i.e., **date_added** and **rating** had very less number of null values, so we removed those null values.\n",
        "4. The 3 columns i.e., **director, cast** and **country** had huge number of null values so dropping these columns couldn't be afforded hence we replaced null values with **\"unknown\"**.\n",
        "5. Converted the data types of features **date_added** and **release_year** to the appropriate data types.\n",
        "6. Renamed name of column **listed_in** to **genre**.\n",
        "7. Broke down the **date_added** column based on **dd/mm/yy** and deleted the previous column.\n",
        "8. We now have **14 rows** instead of **12**.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "netflix.describe()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Treating outlier** is one of the **cautious task** in EDA as removing any uncertain values can misinterpret the data for further observation.\n",
        "- The simplest way to observe outlier in a numeric datatype columns is to implement **describe()** functions which gives min and max values, through which we can compare the difference b/w 2 and then accordingly decide whether we have any outlier or not.\n",
        "- For detecting outliers in categorical dataype columns we have produced certain charts.\n",
        "- Based on the close analysis **no outlier** could be identified.\n",
        "- Hence we can say the dataset is **free from any outlier**."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "one_hot_encoded = pd.get_dummies(netflix, columns=['type'])\n",
        "one_hot_encoded.head()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have applied **One-hot encoding** technique on above dataset.\n",
        "- One of the most common methods, **one-hot encoding**, converts each category value into a new categorical column and assigns a **1 or 0 (True/False)**.\n",
        "- we have applied this technique on **type** column to **distinguish** b/w **type_movie** and **type_tv_show**.\n",
        "- All the shows that falls under respective type of show either **type_movie** or **type_tv_show** will set to **True** or **False**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Contractions are words or combinations of words that are shortened by dropping letters and replacing them with an apostrophe, such as \"don't\" for \"do not\" or \"I'm\" for \"I am\"."
      ],
      "metadata": {
        "id": "eNN9n3-DzhXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "import contractions\n",
        "\n",
        "# Function to expand contractions in a text string\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Apply the function to the description column\n",
        "netflix['expanded_description'] = netflix['description'].apply(expand_contractions)\n",
        "print(netflix[['description', 'expanded_description']].head())\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# List of text columns to convert to lowercase\n",
        "text_columns = ['country', 'description', 'director', 'cast']\n",
        "\n",
        "# Convert each column to lowercase\n",
        "for col in text_columns:\n",
        "    if col in netflix.columns:\n",
        "        netflix[col] = netflix[col].str.lower()\n",
        "    else:\n",
        "        print(f\"Column {col} not found in the dataset.\")\n",
        "\n",
        "print(netflix.head())\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# Define a regex pattern for all punctuation\n",
        "punctuation_pattern = f\"[{string.punctuation}]\"\n",
        "\n",
        "# Columns to clean\n",
        "text_columns = ['title', 'description', 'director', 'cast']\n",
        "\n",
        "# Remove punctuation from these columns\n",
        "for col in text_columns:\n",
        "    if col in netflix.columns:\n",
        "        netflix[col] = netflix[col].str.replace(punctuation_pattern, '', regex=True)\n",
        "    else:\n",
        "        print(f\"Column {col} not found in the dataset.\")\n",
        "\n",
        "print(netflix.head())\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "def remove_words_with_digits(text):\n",
        "    # Pattern to remove words containing digits\n",
        "    word_with_digits_pattern = r'\\w*\\d\\w*'\n",
        "    return re.sub(word_with_digits_pattern, '', text, flags=re.MULTILINE)\n",
        "\n",
        "text_columns = ['description', 'title']\n",
        "\n",
        "for col in text_columns:\n",
        "    if col in netflix.columns:\n",
        "        netflix[col] = netflix[col].astype(str).apply(remove_words_with_digits)\n",
        "    else:\n",
        "        print(f\"Column {col} not found in the dataset.\")\n",
        "\n",
        "print(netflix.head())\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "OYDy2WTv6YZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Join words back into one string separated by space\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Applying the function to the description column\n",
        "netflix['clean_description'] = netflix['description'].astype(str).apply(remove_stopwords)\n",
        "print(netflix[['description', 'clean_description']].head())\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "def remove_whitespace(text):\n",
        "    # Remove leading and trailing whitespaces\n",
        "    text = text.strip()\n",
        "    # Replace internal multiple spaces with a single space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# List of text columns you want to clean (adjust as necessary)\n",
        "text_columns = ['description', 'title', 'director', 'cast']\n",
        "\n",
        "# Apply the whitespace removal function to these columns\n",
        "for col in text_columns:\n",
        "    if col in netflix.columns:\n",
        "        netflix[col] = netflix[col].astype(str).apply(remove_whitespace)\n",
        "    else:\n",
        "        print(f\"Column {col} not found in the dataset.\")\n",
        "\n",
        "print(netflix.head())"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "d-LGhErt-5Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # Use NLTK's word_tokenize to split the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Apply the tokenization function\n",
        "netflix['description_tokens'] = netflix['description'].apply(tokenize_text)\n",
        "print(netflix[['description', 'description_tokens']].head())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def stem_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in text]\n",
        "    return stemmed_tokens\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in text]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Tokenize text (assuming 'description' is the column containing text)\n",
        "netflix['tokens'] = netflix['description'].apply(word_tokenize)\n",
        "netflix['stemmed_tokens'] = netflix['tokens'].apply(stem_text)\n",
        "netflix['lemmatized_tokens'] = netflix['tokens'].apply(lemmatize_text)\n",
        "\n",
        "print(netflix[['tokens', 'stemmed_tokens', 'lemmatized_tokens']].head())"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we have applied both normalization technique i.e., **stemming** and **lemmatizing**.\n",
        "- **Stemming** is the process of reducing words to their root or base form by removing suffixes. For example, \"running\" and \"runs\" would both be reduced to \"run\".\n",
        "- **Lemmatization** is similar to stemming, but it aims to reduce words to their dictionary or lemma form, which is a canonical, meaningful form of the word. For example, \"running\" would be lemmatized to \"run\"."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Apply POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Ensure there are no missing values\n",
        "netflix['description'] = netflix['description'].fillna('')\n",
        "\n",
        "# Apply the function to the description column\n",
        "netflix['pos_tags'] = netflix['description'].apply(pos_tag_text)\n",
        "print(netflix['pos_tags'].head())\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(netflix['description'])\n",
        "\n",
        "# View shape of the TF-IDF matrix\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have taken **TF-IDF vectorization** technique into consideration.\n",
        "- **TF-IDF vectorization** is the process of converting text into numerical data, enabling machine learning algorithms to process text data effectively.\n",
        "- Hence we have a numeric set of data to get a more clear picture of dataset."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = netflix[[\"release_year\",\"year_added\",\"month_added\", \"day_added\"]].corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(corr_matrix)\n",
        "\n",
        "threshold = 0.8\n",
        "high_corr = [(i, j) for i in corr_matrix.columns for j in corr_matrix.columns if (i!=j and abs(corr_matrix[i][j]) > threshold)]\n",
        "print(\"Highly correlated pairs:\", high_corr)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the description as a new feature\n",
        "netflix['description_length'] = netflix['description'].apply(lambda x: len(x.split()))\n",
        "netflix['description_length']"
      ],
      "metadata": {
        "id": "7iqlsFepKBv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PCA\n",
        "features = ['year_added', 'month_added', 'day_added']\n",
        "X = netflix[features]\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA, choose the number of components such as to cover >85% of the variance\n",
        "pca = PCA(n_components=0.85)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Check how many components were selected\n",
        "print(\"Number of components selected:\", pca.n_components_)\n",
        "print(\" \")\n",
        "\n",
        "# Look at the explained variance to understand how much info each component holds\n",
        "print(\"Explained variance by component: %s\" % pca.explained_variance_ratio_)\n",
        "print(\" \")\n",
        "\n",
        "# Create a DataFrame with the loadings\n",
        "loadings = pd.DataFrame(pca.components_.T, columns=['PC%d' % (i + 1) for i in range(pca.n_components_)], index=features)\n",
        "\n",
        "# Display the loadings\n",
        "print(loadings)\n",
        "print(\" \")\n",
        "\n",
        "# Select features based on loadings on the first principal component\n",
        "important_features = loadings.abs().nlargest(5, 'PC1').index.tolist()\n",
        "print(\"Important features based on PCA loadings:\", important_features)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have applied **PCA** to reduce dimensionality in an **unsupervised** manner.\n",
        "- **PCA** is particularly **useful** in **unsupervised scenarios** where a **target variable** is **not defined**. It **reduces** the **dimensionality** of the data by **transforming** the **original variables** into a **new** set of **variables** (principal components), which are linear combinations of the original variables.\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the above analysis we have found **3 features** that have a good **correlation** b/w them.\n",
        "- Since all the feature add important information to the dataset, hence we will consider **all** the **3 features** for analysis."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes, to some extent the data needs to be transformed and we have applied various transformation techniques to the dataset.\n",
        "- We have tried **One hot encoding** feature engineering technique and applied on **type column** of the dataset. This step basically creates 2 new columns with **movie_show** and **tv_show** and sets **True/False** accordingly.\n",
        "- Through **one hot encodin**g technique we have derived a new feature **description_length**.\n",
        "- We have also initiated **PCA** dimensionality reduction technique over **3 columns** and created **3 new features**. Though after analysis we found that all the **3 features** are **crucial** and adds some information.\n"
      ],
      "metadata": {
        "id": "NACts3EoPTie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we have 2 categories of shows tv_show (duration as seasons) and movie (duration as min) so we will pick tv_show for analysis\n",
        "tv_shows_data1 = netflix[netflix[\"type\"]=='TV Show']\n",
        "movie_shows_data1 = netflix[netflix[\"type\"]=='Movie']"
      ],
      "metadata": {
        "id": "rHDZibR0RBOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will then split duration column to extract numeric values\n",
        "tv_shows_data1[[\"seasons\",\"text\"]] = tv_shows_data1[\"duration\"].str.split(\" \", expand=True)\n",
        "tv_shows_data1[\"seasons\"] = tv_shows_data1[\"seasons\"].astype('int')\n",
        "tv_shows_data1.head(1)"
      ],
      "metadata": {
        "id": "Hyey_hlOPdLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "tv_shows_data1[['seasons','description_length']].describe()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "tv_shows_data1[['seasons','description_length']]= min_max_scaler.fit_transform(tv_shows_data1[['seasons','description_length']])\n",
        "\n",
        "print(tv_shows_data1[['seasons','description_length']].head())\n",
        "print(\" \")\n",
        "print(tv_shows_data1[['seasons','description_length']].describe())\n"
      ],
      "metadata": {
        "id": "2e2G3DA2PGku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here first we have done a slight modification in the data to extract out numeric columns and apply scaling technique on it.\n",
        "- We have choosen **Min-Max scaling** technique over here.\n",
        "- The agenda of choosing **min-max scaling** technique is to interpret the data into a scale of **0 to 1** for better visual understanding.\n",
        "- It is **useful** for algorithms that require data within **bounded intervals**."
      ],
      "metadata": {
        "id": "84Nu7jHBT0cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dimensionality reduction** is a critical technique in data preprocessing, especially **when dealing** with **high-dimensional data**.\n",
        "= It **involves reducing** the **number of random variables** under consideration, by obtaining a set of principal variables.\n",
        "- Since **we don't have** a **highly dimensional data**, so we can proceed **without** initiating any dimensionality reduction technique.\n",
        "- **Dimensionality reduction** technique like **PCA** can significantly reduce the outcome and is **difficult to interpret**. So, for a smaller dataset its better not to apply.\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "\n",
        "Although there is no need of \"Dimensionality Reduction\" but still we have applied PCA above.\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Although there is **no need** of **Dimensionality Reduction** but still we have applied **PCA** above.\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since **train-test split** is generally **performed** on **supervised dataset** where we have a **target column** but still just for a visual analysis **we are going** to **perform train-test split** on **above dataset**."
      ],
      "metadata": {
        "id": "HhMfNUh1XwNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# Define the features and the target\n",
        "X = tv_shows_data1.drop('seasons', axis=1)  # Assuming 'seasons' is the target\n",
        "y = tv_shows_data1['seasons']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Train set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)\n",
        "\n",
        "# Check the distribution of the target in the training set\n",
        "sns.histplot(y_train, kde=True, stat=\"density\", linewidth=0)\n",
        "\n",
        "# Check the distribution of the target in the test set\n",
        "sns.histplot(y_test, kde=True, stat=\"density\", linewidth=0, color='red')"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we have done **70-30** ratio **split**. **70%** for **train data** and **30%** for **test data**.\n",
        "- The objective of performing **70-30** ration split is that it **gives better result**.\n",
        "- It is quite visible from the above graph that the **unseen test data** fits considerably **well** with **respect** to **train data**.\n",
        "- Hence we can state that **train data has performed well**."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}